{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfda1339-68fe-416d-b3b8-6349872524b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Long Document Summarization with Llama 3.1 on Bedrock with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353a3bf-98e8-4eb2-a24d-05dd210e6b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "This notebook is meant to demonstrate how you can use the [Llama 3.1 family of models](https://aws.amazon.com/about-aws/whats-new/2024/07/meta-llama-3-1-405b-generally-available-amazon-bedrock/) on Amazon Bedrock for document summarization tasks. \n",
    "\n",
    "All Llama 3.1 models demonstrate significant improvements over previous versions. The models support a 128K context length and exhibit improved reasoning for multilingual dialogue use cases in eight languages. The models access more information from lengthy text to make more informed decisions and leverage richer contextual data to generate more subtle and refined responses. According to Meta, Llama 3.1 405B is one of the best and largest publicly available foundation models and is well suited for synthetic data generation and model distillation. Llama 3.1 models also provide state-of-the-art capabilities in general knowledge, math, tool use, and multilingual translation.\n",
    "\n",
    "This notebook will go through various summarization strategies that will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html), a popular framework for developing applications powered by large language models (LLMs). It will show improvements that Llama 3.1 offers as compared with Llama 3.\n",
    "\n",
    "<b>Note (for reference):</b> Long Document Summarization with using Llama 3 on Bedrock with LangChain is discussed [here](https://github.com/aws-samples/Meta-Llama-on-AWS/blob/main/long-text-summarization/Llama3-Long-Document-summarization-LangChain.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3664e61-3f39-4229-9cf6-26ee090a8608",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "## Llama 3.1 Model Selection\n",
    "\n",
    "There are [three](https://aws.amazon.com/about-aws/whats-new/2024/07/meta-llama-3-1-405b-generally-available-amazon-bedrock/) Llama 3.1 models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Llama 3.1 8B\n",
    "\n",
    "- **Description:** Ideal for limited computational power and resources, faster training times, and edge devices. The model excels at text summarization, text classification, sentiment analysis, and language translation.\n",
    "- **Context Window:** 128k\n",
    "- **Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Text Classification, and Sentiment Analysis.\n",
    "\n",
    "### 2. Llama 3.1 70B\n",
    "\n",
    "- **Description:** Ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. The model excels at text summarization and accuracy, text classification and nuance, sentiment analysis and nuance reasoning, language modeling, dialogue systems, code generation, and following instructions.\n",
    "- **Context Window:** 128k\n",
    "- **Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation.\n",
    "\n",
    "### 2. Llama 3.1 405B\n",
    "\n",
    "- **Description:** Ideal for enterprise level applications, research and development, synthetic data generation, and model distillation. The model excels at general knowledge, long-form text generation, machine translation, enhanced contextual understanding, advanced reasoning and decision making, better handling of ambiguity and uncertainty, increased creativity and diversity, steerability, math, tool use, multilingual translation, and coding.\n",
    "- **Context Window:** 128k\n",
    "- **Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation.\n",
    "\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below summarizes the model performance on the Massive Multitask Language Understanding ([MMLU](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md#instruction-tuned-models)) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Llama 3.1 8B | 69.4%      | \\$0.0003                   | \\$0.0006                    |\n",
    "| Llama 3.1 70B | 83.6%      | \\$0.00265                   | \\$0.0035                     |\n",
    "| Llama 3.1 405B | 87.3%      | \\$0.00532                   | \\$0.016                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Llama 3.1 Model Cards and Prompt Formats](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6389-42ff-405e-8c3b-1855c9db22cf",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed96c9c-58c0-49a8-a032-8b547aa03419",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a29e7-6312-4ac6-a338-a33cbd83b084",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67cb8-1a0c-416c-a8c8-66814a52b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html). <I>We are using Notebooks on SageMaker as it provides the kernel we need to run these examples</I>.\n",
    "2. For Notebook Instance type, choose ml.t3.medium.\n",
    "3. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4415-8fbb-46ac-92e2-3ebcc4888153",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "1. AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "2. [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1ce28-362c-44a7-bbde-fec066fea4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "boto3\n",
    "botocore==1.34.142\n",
    "sqlalchemy==2.0.29\n",
    "pypdf==4.1.0\n",
    "langchain-aws==0.1.6\n",
    "transformers\n",
    "rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad3619-b8c3-49d3-9175-be3013c079e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8017-48a6-4369-a556-fd0ecdbd80f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>NOTE:</b> Restart the kernel with the updated packages that are installed through the dependencies above\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ddb25-b789-4b3c-8928-39f651c98ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c26578-567b-4629-a081-698ff82533fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109e15-ad6f-4a53-a077-5eae296c67ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Initiate the Bedrock Client\n",
    "\n",
    "Import the necessary libraries, along with langchain for bedrock model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee536a3-1cb4-4bab-a8a0-6e41023c9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import rich\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from pypdf import PdfReader\n",
    "\n",
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5aff5b-4b38-48ea-b5ab-5bebc44b1d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set config object and create bedrock runtime client.\n",
    "config = Config(read_timeout=2000)\n",
    "\n",
    "DEFAULT_REGION = \"us-west-2\"\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',\n",
    "                       region_name=DEFAULT_REGION,\n",
    "                       config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876864d8-d99b-45a3-b7e5-c8824ad8e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "<b>NOTE:</b> Ensure that you have access to the Llama 3.1 model you wish to use through Bedrock in the selected region. At the time of writing, Llama 3.1 models are available only in us-west-2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a841ed0-eab4-4921-87b7-a2d07b961405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the desired Llama 3.1 model ID\n",
    "llama3_8b_instruct = \"meta.llama3-8b-instruct-v1:0\"\n",
    "llama3_70b_instruct = \"meta.llama3-70b-instruct-v1:0\"\n",
    "\n",
    "llama3_1_8b_instruct = \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "llama3_1_70b_instruct = \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "llama3_1_405b_instruct = \"meta.llama3-1-405b-instruct-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = llama3_1_70b_instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231ed9-0cac-4fee-b932-059fc253bbf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure LangChain with Boto3\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "With LangChain, you can access Bedrock once you pass the boto3 session information to LangChain. Below, we also specify Meta Llama3.1 405b/70b/8b in `model_id` and pass the Llama3 inference parameters as desired in `model_kwargs`.\n",
    "\n",
    "\n",
    "\n",
    "### Supported parameters\n",
    "\n",
    "The Llama models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"max_gen_len\": int\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bafb1-129d-486b-8c7a-445c8bce1bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For this notebook, we will look at the 70b model in Llama 3 and Llama 3.1 and \n",
    "# review the results. We will use this method to return the ChatBedrock object\n",
    "# corresponding to the appropriate LLM by passing the model Id as string.\n",
    "def GetLLMChatBedrockObject(modelID=DEFAULT_MODEL):\n",
    "    # Instantiate the LangChain ChatBedrock object. This will allow you to use \n",
    "    # LangChain with Chat models on Amazon Bedrock\n",
    "    llm = ChatBedrock(\n",
    "        model_id=modelID,\n",
    "        model_kwargs={\n",
    "            \"max_gen_len\": 2048,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9\n",
    "        },\n",
    "        client=bedrock,\n",
    "    )\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7901c8-7a58-4c98-bdeb-db0dc4d04dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ChatBedrock object for the intended model\n",
    "# llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "\n",
    "# Initialize conversation chain\n",
    "conversation = ConversationChain(\n",
    "    # We set verbose to false to suppress the printing of logs during the\n",
    "    # execution of the conversation chain. This can be set to true when\n",
    "    # you're debugging your conversation chain or trying to understand how\n",
    "    # it is working under the hood.\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38569ac6-5fad-4162-b0c0-c624e742ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#  Generate some text with a prompt.\n",
    "prediction = conversation.predict(input=\"Hi there! How are you doing? Please provide me a trivia question and the answer.\")\n",
    "\n",
    "# Having multilingual support built-in means that you can use Llama 3.1 to write prompts\n",
    "# and receive responses directly in those languages.\n",
    "# Supported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
    "# Few examples below, uncomment a line and run the next cell to see the results.\n",
    "# prediction = conversation.predict(input=\"Hallo! Wie geht es dir? Bitte stellen Sie mir eine Quizfrage und die Antwort.\")  # German\n",
    "# prediction = conversation.predict(input=\"Salut! Comment allez-vous? Veuillez me fournir une question triviale et la réponse.\")  # French\n",
    "# prediction = conversation.predict(input=\"Ciao! Come va? Per favore forniscimi una domanda banale e la risposta.\")  # Italian\n",
    "# prediction = conversation.predict(input=\"Olá! Como vai? Por favor, forneça-me uma pergunta trivial e a resposta.\")  # Portugese\n",
    "# prediction = conversation.predict(input=\"नमस्ते! आप कैसे हैं? कृपया मुझे एक सामान्य प्रश्न और उत्तर प्रदान करें।\")  # Hindi\n",
    "# prediction = conversation.predict(input=\"¡Hola! ¿Cómo estás? Por favor dame una pregunta de trivia y la respuesta.\")  # Spanish\n",
    "# prediction = conversation.predict(input=\"สวัสดี! เป็นอย่างไรบ้าง โปรดให้คำถามและคำตอบเรื่องไม่สำคัญแก่ฉัน\")  # Thai\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dab8e-af09-456d-b6f1-4b11d33b5644",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Document Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23f76d-0a55-490e-8060-a6e821b8eb99",
   "metadata": {},
   "source": [
    "To demonstrate summarization, we will be using an [AWS whitepaper](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf) on architecting HIIPA compliant workloads on AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71e6ec-8500-404c-9dc3-d81b338fcd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's first download the file to build our document store.\n",
    "!mkdir -p ./data\n",
    "\n",
    "urls = [\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AWS-security-whitepaper.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2024, source=filenames[0])\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2214de-07e2-4958-bf06-1339048abceb",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of `DirectoryLoader` from `PyPDF` available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: For the sake of this use-case we are creating chunks of roughly 4000 characters with an overlap of 100 characters using `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d8742-23aa-4814-8e15-1c720ff9234b",
   "metadata": {},
   "source": [
    "#### HIPAA Compliance document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd92e7-cf45-49d3-aa4a-774b70f3e3e0",
   "metadata": {},
   "source": [
    "In this section, we will load the HIPAA compliance document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `hipaa_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930338ac-1950-4e1b-8388-8972ca92e4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Document 1 (HIPAA COMPLIANCE ON AWS)\n",
    "hipaa_documents = []\n",
    "\n",
    "# Load only the first file\n",
    "hipaa_file = filenames[0]\n",
    "hipaa_loader = PyPDFLoader(data_root + hipaa_file)\n",
    "\n",
    "# Here we load a PDF using pypdf into array of documents, where each document contains the page content and metadata with page number.\n",
    "# To access a subset of pages use something like: hipaa_document[70:84], for no of pages, use: len(hipaa_document)\n",
    "hipaa_document = hipaa_loader.load()\n",
    "\n",
    "for idx, hipaa_document_fragment in enumerate(hipaa_document):\n",
    "    hipaa_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    hipaa_documents.append(hipaa_document_fragment)\n",
    "    \n",
    "# Chunking\n",
    "hipaa_doc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a  small chunk size, just to show.\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "hipaa_docs = hipaa_doc_text_splitter.split_documents(hipaa_documents)\n",
    "print(hipaa_docs[0])\n",
    "\n",
    "# Chunked doc count\n",
    "hipaa_chunked_count = len(hipaa_docs)\n",
    "print(\n",
    "    f\"\\nTotal number of pages: {len(hipaa_documents)}\\nNumber of documents chunked and created from the HIPAA Security document: {hipaa_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41056234-520e-441f-abee-13aa85443bf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d90c42-7a08-447d-8143-10571c411c90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarizing Long Documents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410f8c-6d6e-4163-9428-7481abd02ef6",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following sections, we will go over three different summarization techniques with LangChain:\n",
    "    \n",
    " #####   1. Stuff\n",
    " #####   2. Map Reduce\n",
    " #####   3. Refine\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27632-85ba-46cf-bd43-763e328a8001",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Stuff with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b9890-02bf-4927-8fbb-fea029ddc24c",
   "metadata": {},
   "source": [
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want. It is the default way to process documents with an LLM.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is set `stuff` as the `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175ddb7-9a5a-4c9f-8524-0f36a98c8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the LLM object corresponding to the model we need.\n",
    "# llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "    \n",
    "stuff_summary_chain = load_summarize_chain(llm=llm,\n",
    "                                           chain_type=\"stuff\",\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c9859-f01f-4b60-8139-a9b3dd2d0fb2",
   "metadata": {},
   "source": [
    "Next, let's take a look at the Prompt template used by the Stuff summarize chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4690f8f-4883-4291-829c-030374ac3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afeaa3-21ff-4dd0-b9d5-d691e86e2a77",
   "metadata": {},
   "source": [
    "Here, we see that by default, the Prompt template for `llm_chain` has been set to: 'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'\n",
    "\n",
    "This can be altered by instantiating using `from_template` with LangChain to set a new prompt. We can do that below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41262dc2-3d6e-4296-9954-49edb989ec7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt = PromptTemplate.from_template('Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f2796-0dd6-4fc5-9101-fbcdc1e782d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template = stuff_prompt.template #set new prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5e04-8189-4294-8927-5ad4280b82ba",
   "metadata": {},
   "source": [
    "Now that we have set the new prompt template, let us first try generating a summary of the whitepaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232059e-ef40-49b1-9290-14f38f5b1522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# We will get an ERROR in Llama 3 due to the increaesd (128k) context window size.\n",
    "# With Llama 3.1 this cell might take 3-5 minutes to run. \n",
    "try:\n",
    "    stuff_hipaa_summary = stuff_summary_chain.invoke(hipaa_docs)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ae9d5-ca3e-45a1-bac8-15b8b99aff16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(stuff_hipaa_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbe28-64c6-4302-8c42-deba21fa516f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes:\n",
    "In the output for the above cell, we don't get an error despite the context length being longer than 8192 tokens. Refer to the [Llama 3 notebook](https://github.com/aws-samples/Meta-Llama-on-AWS/blob/main/long-text-summarization/Llama3-Long-Document-summarization-LangChain.ipynb), where you will receive an error on this cell. \n",
    "\n",
    "The Llama models have a context length of 128k tokens, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, stuffing will not work. \n",
    "\n",
    "However, note that the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary. Let's explore a couple chunk-wise summarization techniques with [LangChain](https://python.langchain.com/docs/get_started/introduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240a8bd-693c-4152-a186-850fdf1e4f59",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b1252-171a-43c7-a184-41fc47e1b836",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Map Reduce with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef8232-dc84-44c8-8124-4f4ff7e69f64",
   "metadata": {},
   "source": [
    "The `Map_Reduce` method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text. The map reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain method`. What you need to do is set `map_reduce` as the `chain_type` of your chain.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. Model generates individual summaries for all document chunks in parallel\n",
    "4. Reduce all these summaries to a condensed final summary\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/llama3mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e086a-d021-4357-901b-5670731087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the LLM object corresponding to the model we need.\n",
    "llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "# llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes\n",
    "# this to an LLMChain. It then combines and iteratively reduces the mapped \n",
    "# document\n",
    "map_reduce_summary_chain = load_summarize_chain(llm=llm,\n",
    "                                                chain_type=\"map_reduce\",\n",
    "                                                verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae04456-08d3-4f3e-8491-e37d736f18db",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds token_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8de7ca-64d6-4720-9278-a477638b4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation using from_template (recommended)\n",
    "# Sets the prompt template for the summaries generated for all the individual document chunks.\n",
    "initial_map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.llm_chain.prompt.template = initial_map_prompt.template\n",
    "\n",
    "# Sets the prompt template for generating a cumulative summary of all the document chunks for reduce documents chain.\n",
    "reduce_documents_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a detailed summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template = reduce_documents_prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9794d-7ff2-496c-8998-2801b1846a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Map-Reduce`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Map_Reduce works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76502a-13c8-4501-9544-d957ec56bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This cell might take 5-7 minutes to run on Llama 3.\n",
    "# With Llama 3.1 there is about a 1 minute improvement (reduction in run-time). Might take about 4-6 minutes to run.\n",
    "\n",
    "try:\n",
    "    map_reduce_summary = map_reduce_summary_chain.invoke(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fad6e-1ded-4cb3-8da2-1b6bd534b115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(map_reduce_summary['output_text'].strip())\n",
    "# map_reduce_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb754c4-4963-4d59-a816-cfa3363c7432",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "With `Map_Reduce`, the model is able to summarize a large document by overcoming the context limit of Stuffing method with parallel processing. \n",
    "However, it requires multiple calls to the model and potentially loses context between individual summaries of the chunks. To deal with this challenge, let us try another method that performs chunk-wise summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6d9a-7b92-44fc-88c1-aaba42176117",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a9d41-399a-4e9b-928e-97afb4bc68f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Refine with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f8a89-f7e2-411f-b898-a454daf05a29",
   "metadata": {},
   "source": [
    "The `Refine` method is a technique that allows us to recursively summarize our input data. It iteratively updates its answer by looping over the input documents. This method is useful for refining a summary based on new context.`Refine` is a simpler alternative to `Map_Reduce`. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to `Map_Reduce`.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. First chunk is sent to the model; Model returns the corresponding summary\n",
    "4. Langchain gets next chunk and appends it to the returned summary and sends the combined text as a new request to the model; the process repeats until all chunks are processed\n",
    "5. In the end, you have final summary that has been recursively updated using all the document chunks\n",
    "\n",
    "---\n",
    "\n",
    "![refine](imgs/llamarefine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df1a4d-6ab0-4a2a-bed4-228973c532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the LLM object corresponding to the model we need.\n",
    "# llm = GetLLMChatBedrockObject(llama3_70b_instruct)\n",
    "llm = GetLLMChatBedrockObject(llama3_1_70b_instruct)\n",
    "\n",
    "# Run an initial prompt on a small chunk of data to generate a summary.\n",
    "# Then, for each subsequent document, the output from the previous document is\n",
    "# passed in along with the new document, and the LLM is asked to refine the\n",
    "# output based on the new document.\n",
    "refine_summary_chain = load_summarize_chain(llm=llm,\n",
    "                                            chain_type=\"refine\",\n",
    "                                            verbose=False)\n",
    "\n",
    "# Refine summary chain for summarization\n",
    "refine_summary_chain_french = load_summarize_chain(llm=llm,\n",
    "                                                   chain_type=\"refine\",\n",
    "                                                   verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf21cbc-1b13-4341-b00a-300b579ddcfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Refine`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Refine works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e05ef-8a19-415c-ac84-41bac282f1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial llm chain prompt template\n",
    "initial_refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "refine_summary_chain.initial_llm_chain.prompt.template = initial_refine_prompt.template\n",
    "\n",
    "# Refine llm chain prompt template\n",
    "refine_documents_prompt = PromptTemplate.from_template(\n",
    "    \"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain.refine_llm_chain.prompt.template = refine_documents_prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58d720-bccb-4bee-b836-020d6ddf56ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This cell might take 8-10 minutes to run on Llama 3.\n",
    "# With Llama 3.1 there is about a 2 minute improvement (reduction in run-time). Might take about 6-8 minutes to run.\n",
    "try:\n",
    "    refine_summary = refine_summary_chain.invoke(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584649c-fd50-4527-a04f-8c4f1a4d25ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(refine_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe47f1-a1cc-4a06-97ff-6494256f8ef7",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "`Refine` has the potential to incorporate more relevant context compared to `Map_Reduce`, potentially resulting in a more comprehensive and accurate summary. However, it comes with a trade-off: `Refine` necessitates a significantly higher number of calls to the LLM than the `Stuff` and `Map_Reduce` since it is an incremental process where the subsequent chunk's summary uses the previous chunk's summary. Moreover, these calls are not independent, which means they cannot be parallelized, potentially leading to longer processing times. Another consideration is that the Refine method may exhibit recency bias, where the most recent document chunks in the sequence could carry more weight or influence in the final summary, as the method processes documents in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e1607-ee27-4c96-bf9a-3772201dbb21",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f979c-4d80-408d-9799-20043d9fbe0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we looked at three different summarization techniques using LangChain; **Stuff**, **Map_Reduce**, and **Refine**. Each of these methods has its own distinct advantages/uses. \n",
    "\n",
    "- ***Stuff*** is straighforward and is the fastest method out of the three since it makes a single call to the LLM and fits the entire document within the model's context window. Although as we saw with the HIPAA Compliance document, it does not scale well to work with large volumes of text.\n",
    "\n",
    "- ***Map_Reduce*** deals with the issue of the context window length while being able to parallelize generation of summaries for individual chunks, thereby speeding up the model's response while being able to process long documents. An issue with Map_Reduce is that since this is not a recursive process, we lose context between chunks during this process.\n",
    "\n",
    "- ***Refine*** deals with the issues that arise with the previous methods. It performs recursive summarization by incrementally generating summaries for each of the chunks while retaining context between them. While this method generates the most accurate and comprehensive summary out of all 3 methods, the calls made to the LLM cannot be parallelized. This can result in longer processing times. Additionally, more recent document chunks tend to carry more weight due to the order that they are processed in.\n",
    "\n",
    "We can see Llama 3.1 shows improvements in latency compared to Llama 3 on the same tasks owning to its larger context window (128k for Llama 3.1 as compared to 8k for Llama 3). With Refine technique we saw a larger latency improvement as compared to Map Reduce technique. This is just one aspect of comparison. For your usecase consider the approriate chunking strategy based on holistic analysis considering all other aspects as well. \n",
    "\n",
    "We also saw that given the direct support to 8 languages, it also allows you to prompt and get results in multiple languages. This can help you build powerful user experiences with multilingual chat interfaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca227dd3-5b63-4fd7-bf48-38bd36252a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Meta\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
