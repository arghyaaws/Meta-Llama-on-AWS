{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers  sagemaker seaborn sentence-transformers nltk scikit-learn \"huggingface_hub[cli]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3 finetuning for Bedrock\n",
    "## Architecture\n",
    "This diagram illustrates an end-to-end ML workflow where a SageMaker Pipeline processes, trains, and evaluates a model using HuggingFace containers, then registers it before deploying to Amazon Bedrock through a Lambda function for inference, with model artifacts stored in S3 throughout the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture Diagram](Llama3_finetuning_bedrock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The next line of code uses a API token to login in the Huggingface account to use the model weights. You need to have access to \"meta-llama/Llama-3.2-3B-Instruct\" to use meta llama 3.2 3B model.\n",
    "- [Hugging Face Access Tokens Documentation](https://huggingface.co/docs/hub/en/security-tokens).\n",
    "- [Getting access to the mode](https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/172)\n",
    "- [meta llama 3.2 3B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_token='HF-token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token {huggingface_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    #use this code if you are running locally\n",
    "    sagemaker_execution_role = \"Update-with-your-current-AmazonSageMaker-ExecutionRole\"\n",
    "    role = iam.get_role(RoleName=sagemaker_execution_role)['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client('sagemaker', region_name=sess.boto_region_name)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_S3Uri=\"s3://jumpstart-cache-prod-us-west-2/training-datasets/oasst_top/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = S3Downloader.download(s3_uri=dataset_S3Uri, local_path=f\"dataset/\")\n",
    "print(f\"Training config downloaded to:\")\n",
    "print(train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "input_path = f's3://{sess.default_bucket()}/datasets/llama3'\n",
    "# upload the model yaml file to s3\n",
    "train_dataset_path = \"dataset/train.jsonl\"\n",
    "train_s3_path = S3Uploader.upload(local_path=train_dataset_path, desired_s3_uri=f\"{input_path}/dataset\")\n",
    "\n",
    "print(f\"Training dataset uploaded to:\")\n",
    "print(train_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CacheConfig\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "#sagemaker_session = sagemaker.Session()\n",
    "#role = sagemaker.get_execution_role()\n",
    "# Define pipeline parameters\n",
    "region=sagemaker_session.boto_region_name\n",
    "model_name = \"llama3-qa-model\"\n",
    "instance_type_preprocessing = \"ml.m5.large\"\n",
    "instance_count = 1\n",
    "# Cache configuration to improve pipeline execution time\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    instance_type=instance_type_preprocessing,\n",
    "    instance_count=instance_count,\n",
    "    base_job_name=\"llama3-qa-preprocessing\",\n",
    "    role=role,\n",
    "    max_runtime_in_seconds=3600,  # Set a maximum runtime of 1 hour,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "inputs = [\n",
    "    ProcessingInput(source=train_s3_path, destination=\"/opt/ml/processing/input\"),\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\"),\n",
    "    ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"PreprocessQADataset\",\n",
    "    processor=preprocessing_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    \n",
    "    code=\"scripts/preprocessing/preprocess.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_3_2_3B_fsdp_lora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Llama-3.2-3B-Instruct\"# Hugging Face model id\n",
    "max_seq_length:  512 #2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test\"   # path to where SageMaker saves test dataset\n",
    "#output_dir: \"/opt/ml/model\"            # path to where SageMaker will upload the model \n",
    "output_dir: \"/tmp/llama3\"            # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 10                   # number of training epochs\n",
    "per_device_train_batch_size: 16         # batch size per device during training\n",
    "per_device_eval_batch_size: 16          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: false                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"llama_3_2_3B_fsdp_lora.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import time\n",
    "\n",
    "# define Training Job Name with timestamp\n",
    "\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "job_name = f'llama3-8B-exp1-{timestamp}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'training/train_fsdp_lora.py',      # train script\n",
    "    model_dir            = '/opt/ml/model',\n",
    "    source_dir           = 'scripts/',  # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',  # instances type used for the training job\n",
    "    #instance_type        = 'ml.g5.48xlarge',  # instances type used for the training job\n",
    "    #instance_type        = 'ml.g5.16xlarge',  # instances type used for the training job\n",
    "    instance_count       = 2,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 500,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/llama_3_2_3B_fsdp_lora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    sagemaker_session=pipeline_session,\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    "    \n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=job_name,\n",
    "    estimator=huggingface_estimator,\n",
    "    inputs={\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "        ),\n",
    "        \"config\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=train_config_s3_path,\n",
    "        ),\n",
    "        \"test\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Register Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\",\n",
    "  region=region,\n",
    "  version=\"2.0\",\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "llm_model=HuggingFaceModel(\n",
    "    transformers_version=\"4.37.0\",\n",
    "    pytorch_version=\"1.10.2\",\n",
    "    py_version=\"py310\",\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model step\n",
    "llama_model_step = CreateModelStep(\n",
    "    name=\"CreateLlama3ModelStep\",\n",
    "    model=llm_model,\n",
    "    inputs=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    depends_on=[training_step],\n",
    ")\n",
    "    \n",
    "# Crete a RegisterModel step, which registers the model with Sagemaker Model Registry.\n",
    "model_package_group_name = \"Llama3Models\" \n",
    "step_register_model = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    model=llm_model,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.g5.12xlarge\"],\n",
    "    transform_instances=[\"ml.g5.12xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    depends_on=[training_step],\n",
    "    approval_status=\"Approved\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock Deployment step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('boto3-layer/python', exist_ok=True)\n",
    "\n",
    "# Install boto3 into the layer directory\n",
    "subprocess.check_call([\n",
    "    'pip', 'install', 'boto3==1.35.16', '-t', 'boto3-layer/python',\n",
    "    '--upgrade', '--no-cache-dir'\n",
    "])\n",
    "\n",
    "# Create zip file\n",
    "shutil.make_archive('boto3-layer', 'zip', 'boto3-layer')\n",
    "\n",
    "# Upload to AWS as a Lambda layer\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "with open('boto3-layer.zip', 'rb') as zip_file:\n",
    "    response = lambda_client.publish_layer_version(\n",
    "        LayerName='boto3-latest',\n",
    "        Description='Latest Boto3 layer',\n",
    "        Content={\n",
    "            'ZipFile': zip_file.read()\n",
    "        },\n",
    "        CompatibleRuntimes=['python3.10', 'python3.11']\n",
    "    )\n",
    "\n",
    "print(f\"Layer ARN: {response['LayerArn']}\")\n",
    "print(f\"Layer Version ARN: {response['LayerVersionArn']}\")\n",
    "lambda_layer_arn=response['LayerVersionArn']\n",
    "\n",
    "# Clean up\n",
    "shutil.rmtree('boto3-layer')\n",
    "os.remove('boto3-layer.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Role and policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_execution_role(role_name, training_bucket, account_id, region):\n",
    "    iam = boto3.client('iam')\n",
    "    \n",
    "    # Define the trust relationship\n",
    "    trust_relationship = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"lambda.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:SourceAccount\": account_id\n",
    "                    },\n",
    "                    \"ArnEquals\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define Bedrock permissions policy\n",
    "    bedrock_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:CreateModelImportJob\",\n",
    "                    \"bedrock:GetModelImportJob\",\n",
    "                    \"bedrock:ListModelImportJobs\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"iam:PassRole\"\n",
    "                ],\n",
    "                \"Resource\": f\"arn:aws:iam::{account_id}:role/*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"iam:PassedToService\": \"bedrock.amazonaws.com\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Define S3 permissions\n",
    "    s3_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{training_bucket}\",\n",
    "                    f\"arn:aws:s3:::{training_bucket}/*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def attach_policies(role_name):\n",
    "        # Attach the Bedrock permissions policy\n",
    "        iam.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName='BedrockAccessPolicy',\n",
    "            PolicyDocument=json.dumps(bedrock_policy)\n",
    "        )\n",
    "        print(\"Attached Bedrock permissions policy\")\n",
    "        \n",
    "        # Attach necessary AWS managed policies for Lambda basic execution\n",
    "        try:\n",
    "            iam.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn=\"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "            )\n",
    "            print(\"Attached Lambda basic execution policy\")\n",
    "        except iam.exceptions.EntityAlreadyExistsException:\n",
    "            print(\"Lambda basic execution policy already attached\")\n",
    "        \n",
    "        # Attach S3 policy\n",
    "        iam.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName='S3AccessPolicy',\n",
    "            PolicyDocument=json.dumps(s3_policy)\n",
    "        )\n",
    "        print(\"Attached S3 access policy\")\n",
    "    \n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_relationship),\n",
    "            Description=\"Execution role for Lambda function and Bedrock model import jobs\"\n",
    "        )\n",
    "        \n",
    "        role_arn = response['Role']['Arn']\n",
    "        print(f\"Created IAM role: {role_arn}\")\n",
    "        \n",
    "        # Attach all policies for new role\n",
    "        attach_policies(role_name)\n",
    "        \n",
    "        return role_arn\n",
    "    \n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f\"IAM role {role_name} already exists. Retrieving its ARN.\")\n",
    "        role = iam.get_role(RoleName=role_name)\n",
    "        \n",
    "        # Update the trust relationship\n",
    "        iam.update_assume_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyDocument=json.dumps(trust_relationship)\n",
    "        )\n",
    "        \n",
    "        # Attach or update policies for existing role\n",
    "        attach_policies(role_name)\n",
    "        \n",
    "        return role['Role']['Arn']\n",
    "\n",
    "# Usage\n",
    "role_name = \"LambdaBedrockExecutionRole\"\n",
    "training_bucket = sagemaker_session_bucket\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = \"us-west-2\"\n",
    "execution_role_arn = create_lambda_execution_role(role_name, training_bucket, account_id, region)\n",
    "print(f\"Execution Role ARN: {execution_role_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def handle_client_error(func, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "def create_or_update_role(role_name, trust_relationship, permission_policy, iam_client=None, account_id=None):\n",
    "    iam = iam_client or boto3.client('iam')\n",
    "    account_id = account_id or boto3.client('sts').get_caller_identity()['Account']\n",
    "    \n",
    "    # Check and update/create role\n",
    "    role = handle_client_error(iam.get_role, RoleName=role_name)\n",
    "    if role:\n",
    "        iam.update_assume_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyDocument=json.dumps(trust_relationship)\n",
    "        )\n",
    "        print(f\"Updated existing role: {role_name}\")\n",
    "    else:\n",
    "        iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_relationship)\n",
    "        )\n",
    "        print(f\"Created new role: {role_name}\")\n",
    "\n",
    "    # Handle policy\n",
    "    policy_name = f\"{role_name}Policy\"\n",
    "    policy_arn = f\"arn:aws:iam::{account_id}:policy/{policy_name}\"\n",
    "    \n",
    "    # Attach or update policy\n",
    "    policy = handle_client_error(iam.get_policy, PolicyArn=policy_arn)\n",
    "    if policy:\n",
    "        iam.create_policy_version(\n",
    "            PolicyArn=policy_arn,\n",
    "            PolicyDocument=json.dumps(permission_policy),\n",
    "            SetAsDefault=True\n",
    "        )\n",
    "        # Cleanup old versions\n",
    "        versions = iam.list_policy_versions(PolicyArn=policy_arn)['Versions']\n",
    "        for version in versions:\n",
    "            if not version['IsDefaultVersion']:\n",
    "                iam.delete_policy_version(\n",
    "                    PolicyArn=policy_arn,\n",
    "                    VersionId=version['VersionId']\n",
    "                )\n",
    "        print(f\"Updated existing policy: {policy_name}\")\n",
    "    else:\n",
    "        iam.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=json.dumps(permission_policy)\n",
    "        )\n",
    "        print(f\"Created new policy: {policy_name}\")\n",
    "\n",
    "    # Attach policy to role if not already attached\n",
    "    attached_policies = iam.list_attached_role_policies(RoleName=role_name)['AttachedPolicies']\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=policy_arn\n",
    "    )\n",
    "    print(f\"Attached policy to role: {role_name}\")\n",
    "\n",
    "    return iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "\n",
    "\n",
    "\n",
    "# Set up variables\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = \"us-west-2\"\n",
    "training_bucket = sagemaker_session_bucket\n",
    "role_name = \"Sagemaker_Bedrock_import_role\"\n",
    "\n",
    "# Define policies\n",
    "trust_relationship = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"},\n",
    "        \"Action\": \"sts:AssumeRole\",\n",
    "        \"Condition\": {\n",
    "            \"StringEquals\": {\"aws:SourceAccount\": account_id},\n",
    "            \"ArnEquals\": {\"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"}\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "\n",
    "permission_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n",
    "        \"Resource\": [f\"arn:aws:s3:::{training_bucket}\", f\"arn:aws:s3:::{training_bucket}/*\"],\n",
    "        \"Condition\": {\"StringEquals\": {\"aws:ResourceAccount\": account_id}}\n",
    "    }]\n",
    "}\n",
    "\n",
    "# Create or update the role\n",
    "role_arn = create_or_update_role(role_name, trust_relationship, permission_policy)\n",
    "print(f\"Role ARN: {role_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lambda_helper import Lambda\n",
    "# Create Lambda function instance\n",
    "lambda_func = Lambda(\n",
    "    function_name=\"bedrock-model-import\",\n",
    "    execution_role_arn=execution_role_arn,\n",
    "    script=\"scripts/lambda/bedrock_model_import.py\",\n",
    "    handler='bedrock_model_import.lambda_handler',\n",
    "    timeout=900,  # 15 minutes, adjust as needed\n",
    "    memory_size=128,\n",
    "    runtime='python3.12',\n",
    "    layers=[lambda_layer_arn],  # Your boto3 layer ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.lambda_step import LambdaStep, LambdaOutput, LambdaOutputTypeEnum\n",
    "# Define the outputs\n",
    "lambda_outputs = [\n",
    "    LambdaOutput(output_name=\"model_arn\", output_type=LambdaOutputTypeEnum.String)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_register_model.properties.ModelPackageArn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Lambda step\n",
    "lambda_step = LambdaStep(\n",
    "    name=\"BedrockModelImport\",\n",
    "    lambda_func=lambda_func,\n",
    "    inputs={\n",
    "        \"model_uri\": training_step.properties.ModelArtifacts.S3ModelArtifacts,  # Use the output from the training step\n",
    "        \"role_arn\": role,\n",
    "        \"model_name\": model_name\n",
    "    },\n",
    "    outputs=lambda_outputs,\n",
    "    cache_config=CacheConfig(enable_caching=True, expire_after=\"1d\"),\n",
    "    depends_on=[step_register_model]\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\"model_name\": \"llama3_model\",\n",
    "\"model_uri\":training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "\"role_arn\": role_arn,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    pipeline = Pipeline(\n",
    "        name=\"Llama3-QAPipeline\",\n",
    "        steps=[preprocessing_step, training_step,step_register_model,lambda_step ],\n",
    "        parameters=[role, model_name],\n",
    "        sagemaker_session=pipeline_session,\n",
    "    )\n",
    "    logging.info(\"Pipeline created successfully\")\n",
    "\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    logging.info(\"Pipeline upserted successfully\")\n",
    "\n",
    "    execution = pipeline.start()\n",
    "    logging.info(\"Pipeline started successfully\")\n",
    "\n",
    "except ValueError as ve:\n",
    "    logging.error(f\"ValueError occurred: {str(ve)}\")\n",
    "    logging.error(f\"Error occurred in pipeline definition: {pipeline.definition()}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\")\n",
    "    logging.error(f\"Error type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_pipeline_status(execution):\n",
    "    try:\n",
    "        return execution.describe()['PipelineExecutionStatus']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting pipeline status: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_step_statuses(execution):\n",
    "    try:\n",
    "        steps = execution.list_steps()\n",
    "        return {step['StepName']: step['StepStatus'] for step in steps}\n",
    "    except ClientError as e:\n",
    "        print(f\"Error getting step statuses: {e}\")\n",
    "        return {}\n",
    "\n",
    "def is_pipeline_finished(status):\n",
    "    return status in ['Succeeded', 'Completed', 'Failed', 'Stopped']\n",
    "\n",
    "def print_progress(status, step_statuses):\n",
    "    print(f\"\\nPipeline status: {status}\")\n",
    "    print(\"Step statuses:\")\n",
    "    for step, status in step_statuses.items():\n",
    "        print(f\"  {step}: {status}\")\n",
    "\n",
    "def monitor_pipeline_execution(execution, check_interval=60):\n",
    "    print(\"Pipeline execution started.\")\n",
    "    print(\"Status updates (checking every minute):\")\n",
    "\n",
    "    previous_step_statuses = {}\n",
    "    while True:\n",
    "        status = get_pipeline_status(execution)\n",
    "        if status is None:\n",
    "            print(\"Failed to get pipeline status. Retrying...\")\n",
    "            time.sleep(check_interval)\n",
    "            continue\n",
    "\n",
    "        step_statuses = get_step_statuses(execution)\n",
    "        \n",
    "        if step_statuses != previous_step_statuses:\n",
    "            print_progress(status, step_statuses)\n",
    "            previous_step_statuses = step_statuses\n",
    "        else:\n",
    "            print(\".\", end='', flush=True)\n",
    "        \n",
    "        if is_pipeline_finished(status):\n",
    "            break\n",
    "\n",
    "        time.sleep(check_interval)\n",
    "\n",
    "    print(\"\\nPipeline execution finished.\")\n",
    "    print_progress(status, step_statuses)\n",
    "\n",
    "# Usage example:\n",
    "monitor_pipeline_execution(execution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
